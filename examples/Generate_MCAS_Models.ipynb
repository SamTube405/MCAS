{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import dtw\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import copy\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics\n",
    "'''\n",
    "\n",
    "def myerrsq(x,y):\n",
    "    return((x-y)**2)\n",
    "\n",
    "### s2 predictions, s1 ground truth\n",
    "def dtw_(s1, s2):\n",
    "    window=2\n",
    "    \n",
    "    s1= pd.DataFrame(s1)\n",
    "    s2 = pd.DataFrame(s2)\n",
    "    \n",
    "    z1=(s1-s1.mean())/(s1.std(ddof=0).apply(lambda m: (m if m > 0.0 else 1.0)))\n",
    "    z2=(s2-s2.mean())/(s2.std(ddof=0).apply(lambda m: (m if m > 0.0 else 1.0)))\n",
    "\n",
    "    ### first value simulation second value GT\n",
    "    dtw_metric = np.sqrt(dtw.dtw(z2[0], z1[0], dist_method=myerrsq, window_type='slantedband',\n",
    "                               window_args={'window_size':window}).normalizedDistance)\n",
    "    \n",
    "    return dtw_metric\n",
    "\n",
    "def ae(v1,v2):\n",
    "    v1=np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    return np.abs(v1 - v2)\n",
    "\n",
    "# Scale-Free Absolute Error\n",
    "def sfae(v1,v2):\n",
    "    \n",
    "    v1=np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    \n",
    "    return ae(v1, v2) / np.mean(v1)\n",
    "\n",
    "def MAD_mean_ratio(v1, v2):\n",
    "    \"\"\"\n",
    "    MAD/mean ratio\n",
    "    \"\"\"\n",
    "    return np.mean(sfae(v1, v2))\n",
    "\n",
    "def normed_rmse(v1,v2):\n",
    "    v1=np.cumsum(v1)\n",
    "    v2=np.cumsum(v2)\n",
    "    v1=v1/np.max(v1)\n",
    "    v2=v2/np.max(v2)\n",
    "    \n",
    "    result = v1-v2\n",
    "    result = (result ** 2).mean()\n",
    "    result = np.sqrt(result)\n",
    "    return result\n",
    "\n",
    "def rmse(v1,v2):\n",
    "    result = np.array(v1)-np.array(v2)\n",
    "    result = (result ** 2).mean()\n",
    "    result = np.sqrt(result)\n",
    "    return result\n",
    "\n",
    "def ape(v1,v2):\n",
    "    v1=np.sum(v1)\n",
    "    v2=np.sum(v2)\n",
    "    result = np.abs(float(v1) - float(v2))\n",
    "    result = 100.0 * result / np.abs(float(v1))\n",
    "    return result\n",
    "\n",
    "def smape(A, F):\n",
    "    A=np.array(A)\n",
    "    F=np.array(F)\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_predictions(model_id,gt_data, sim_data, narrative_list=[]):\n",
    "    '''\n",
    "    Description: Run predictions for LSTM models\n",
    "    \n",
    "    Input:\n",
    "    model_id: \n",
    "    gt_data: ground truth count dict\n",
    "    sim_data: simulation count dict\n",
    "    target: target dataframe\n",
    "    narrative_list: list of info ids\n",
    "    '''\n",
    "    Gperformance_data=[]\n",
    "\n",
    "    for Tnarrative in narrative_list:\n",
    "        Tnarrative = Tnarrative.replace(\"informationID_\", \"\")\n",
    "        sim_y=gt_data[Tnarrative]\n",
    "        y_hat=sim_data[Tnarrative]\n",
    "        \n",
    "        print(\"Test on narrative: %s\"%Tnarrative)\n",
    "        ape_value=ape(sim_y,y_hat)\n",
    "        print(\"APE: \",ape_value)\n",
    "\n",
    "        rmse_value=rmse(sim_y,y_hat)\n",
    "        print(\"RMSE: \",rmse_value)\n",
    "\n",
    "        nrmse_value=normed_rmse(sim_y,y_hat)\n",
    "        print(\"NRMSE: \",nrmse_value)\n",
    "\n",
    "        smape_value=smape(sim_y,y_hat)\n",
    "        print(\"SMAPE: \",smape_value)\n",
    "        \n",
    "        sfae_value=MAD_mean_ratio(sim_y,y_hat)\n",
    "        print(\"SFAE: \",sfae_value)\n",
    "        \n",
    "        dtw_value = dtw_(sim_y, y_hat)\n",
    "        print(\"DTW :\", dtw_value)\n",
    "\n",
    "        Gperformance_data.append([Tnarrative,ape_value,rmse_value,nrmse_value,smape_value,sfae_value,dtw_value,model_id])\n",
    "\n",
    "    Gperformance_data=pd.DataFrame(Gperformance_data,columns=['informationID','APE','RMSE','NRMSE','SMAPE','SFAE','DTW','MODEL'])\n",
    "    \n",
    "    return Gperformance_data\n",
    "\n",
    "### Returns validation performance, simulation performance, validation data and simulation data\n",
    "def helper_performance(paths = [], start_sim_period=\"\", end_sim_period=\"\", evaluation=True):\n",
    "    ### Gperformance plots.\n",
    "    performance_data=[]\n",
    "    performance_data_eval=[]\n",
    "    sim_data={}\n",
    "    val_data={}\n",
    "\n",
    "    ### Iterate over all model paths in a list to load and compute performance metrics (including internal model)\n",
    "    for path in paths:\n",
    "        xmodel_name_array=path.split(\"/\")[-1].split(\"_\")\n",
    "        xmodel_name = xmodel_name_array[-2]\n",
    "        window = xmodel_name_array[-3]\n",
    "        xmodel_name_ = xmodel_name + \"_\" + window\n",
    "        \n",
    "        print(xmodel_name_array)\n",
    "        if len(xmodel_name_array)>3:\n",
    "            if (xmodel_name_array[1]==start_sim_period) & (xmodel_name_array[2]==end_sim_period):\n",
    "                print(\"Into the blender: \",path)\n",
    "                ### Read validation performance\n",
    "                Gperformance=pd.read_pickle(path+'/Gperformance_validation.pkl.gz')\n",
    "                Gperformance['MODEL_ID']=xmodel_name_\n",
    "                Gperformance[\"MODEL\"] = xmodel_name\n",
    "                Gperformance[\"FILE_NAME\"] = path\n",
    "                if evaluation:\n",
    "                    Gperformance_eval=pd.read_pickle(path+'/Gperformance_simulation.pkl.gz')\n",
    "                    Gperformance_eval['MODEL_ID']=xmodel_name_\n",
    "                    Gperformance_eval[\"MODEL\"] = xmodel_name\n",
    "                    Gperformance_eval[\"FILE_NAME\"] = path\n",
    "\n",
    "                with open(path+'/simulations_data.pkl.gz', 'rb') as fd:\n",
    "                    sim_data_=pickle.load(fd)\n",
    "                    sim_data.setdefault(xmodel_name_,sim_data_)\n",
    "\n",
    "                ### Read validation data predictions\n",
    "                with open(path+'/validation_data.pkl.gz', 'rb') as fd:\n",
    "                    val_data_=pickle.load(fd)\n",
    "                    val_data.setdefault(xmodel_name_,val_data_)\n",
    "\n",
    "                ### read ground truth simulation data\n",
    "                if evaluation:\n",
    "                    with open(path+'/gt_data_simulations.pkl.gz', 'rb') as fd:\n",
    "                        gt_data=pickle.load(fd)\n",
    "\n",
    "                    ### read ground truth validation data\n",
    "                    with open(path+'/gt_data_validation.pkl.gz', 'rb') as fd:\n",
    "                        gt_data_val=pickle.load(fd)\n",
    "\n",
    "                ### add new results to df and add to list\n",
    "                if evaluation:\n",
    "                    performance_data_eval.append(Gperformance_eval)\n",
    "                performance_data.append(Gperformance)\n",
    "\n",
    "    performance_data=pd.concat(performance_data, ignore_index=True)\n",
    "    if evaluation:\n",
    "        performance_data_eval=pd.concat(performance_data_eval, ignore_index=True)\n",
    "        return performance_data, performance_data_eval, val_data, sim_data\n",
    "    else:\n",
    "        return performance_data, val_data, sim_data\n",
    "    \n",
    "def ensemble_all_average(paths, gt_path=\"\",model_id=\"\", evaluation=True):\n",
    "    sim_ensemble = {}\n",
    "    \n",
    "    if evaluation:\n",
    "        with open(gt_path+'/gt_data_simulations.pkl.gz', 'rb') as fd:\n",
    "                gt=pickle.load(fd)\n",
    "    \n",
    "    for path in paths:\n",
    "        \n",
    "        with open(path+'/simulations_data.pkl.gz', 'rb') as fd:\n",
    "            sim_data=pickle.load(fd)\n",
    "            \n",
    "        for k,v in sim_data.items():\n",
    "            if k in sim_ensemble:\n",
    "                arr = sim_ensemble[k]\n",
    "                arr.append(list(v))\n",
    "                sim_ensemble[k] = arr\n",
    "            else:\n",
    "                sim_ensemble[k] = [list(v)]\n",
    "        \n",
    "    sim_ensemble_final = {}\n",
    "    for k, v in sim_ensemble.items():\n",
    "\n",
    "        v_np = np.array(v)\n",
    "        sim_ensemble_final[k] = np.round(np.mean(v, axis=0))\n",
    "    \n",
    "    infoids = list(sim_ensemble_final.keys())\n",
    "    \n",
    "    if evaluation:\n",
    "        # Evaluate simulations\n",
    "        Gperformance = eval_predictions(model_id, gt, sim_ensemble_final, infoids)\n",
    "    \n",
    "        return Gperformance, sim_ensemble_final\n",
    "    else:\n",
    "        return sim_ensemble_final\n",
    "    \n",
    "def run_blender(val_perf, sim_data, sim_perf=\"\", metric=\"RMSE\", evaluation=True, model_id=\"\"):\n",
    "    \n",
    "    ### Get minimum in validation based on metric\n",
    "    idx = val_perf.groupby(['informationID'])[metric].transform(min) == val_perf[metric]\n",
    "    val_perf_blended=val_perf[idx]\n",
    "    val_perf_blended=val_perf_blended.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    if evaluation:\n",
    "        \n",
    "        sim_perf['unique_id'] = sim_perf['MODEL_ID']+\"_\"+sim_perf['informationID']\n",
    "        val_perf_blended['unique_id'] = val_perf_blended['MODEL_ID']+\"_\"+val_perf_blended['informationID']\n",
    "        xmodel_ids = list(val_perf_blended['unique_id'].unique())\n",
    "        sim_perf = sim_perf.loc[sim_perf['unique_id'].isin(xmodel_ids)].reset_index(drop=True)\n",
    "        sim_perf.drop(columns=['unique_id'], inplace=True)\n",
    "        \n",
    "    sim_data_blended={}\n",
    "    def getBlendedSimulation(row):\n",
    "        sim_array=sim_data[row['MODEL_ID']][row['informationID']]\n",
    "        print(row['MODEL_ID'],row['informationID'],sim_array)\n",
    "        sim_data_blended.setdefault(row['informationID'],sim_array)\n",
    "    print(\"\\n\\n Blended simulation output ---------------\")    \n",
    "    T=val_perf_blended.apply(getBlendedSimulation,axis=1)\n",
    "    \n",
    "    if evaluation:\n",
    "        sim_perf[\"MODEL\"] = model_id\n",
    "        return val_perf_blended, sim_perf, sim_data_blended\n",
    "    else:\n",
    "        return val_perf_blended, sim_data_blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Returns validation performance, simulation performance, validation data and simulation data\n",
    "def get_validation_data(paths = [], start_sim_period=\"\", end_sim_period=\"\", simulation=True):\n",
    "    ### Gperformance plots.\n",
    "    sim_data={}\n",
    "    val_data={}\n",
    "    gt_data_val = {}\n",
    "    gt_data = 0\n",
    "\n",
    "    ### Iterate over all model paths in a list to load and compute performance metrics (including internal model)\n",
    "    for path in paths:\n",
    "        xmodel_name_array=path.split(\"/\")[-1].split(\"_\")\n",
    "        xmodel_name = xmodel_name_array[-2]\n",
    "        window = xmodel_name_array[-3]\n",
    "        xmodel_name_ = xmodel_name + \"_\" + window\n",
    "        \n",
    "        print(xmodel_name_array)\n",
    "        if len(xmodel_name_array)>3:\n",
    "            if (xmodel_name_array[1]==start_sim_period) & (xmodel_name_array[2]==end_sim_period):\n",
    "                print(\"For the metalearner: \",path)\n",
    "\n",
    "                with open(path+'/simulations_data.pkl.gz', 'rb') as fd:\n",
    "                    sim_data_=pickle.load(fd)\n",
    "                    sim_data.setdefault(xmodel_name_,sim_data_)\n",
    "\n",
    "                ### Read validation data predictions\n",
    "                with open(path+'/validation_data.pkl.gz', 'rb') as fd:\n",
    "                    val_data_=pickle.load(fd)\n",
    "                    val_data.setdefault(xmodel_name_,val_data_)\n",
    "\n",
    "                if simulation:\n",
    "                    gt_data = 0\n",
    "                else:\n",
    "                    ### read ground truth simulation data\n",
    "                    with open(path+'/gt_data_simulations.pkl.gz', 'rb') as fd:\n",
    "                        gt_data=pickle.load(fd)\n",
    "\n",
    "                ### read ground truth validation data\n",
    "                with open(path+'/gt_data_validation.pkl.gz', 'rb') as fd:\n",
    "                    gt_data_val=pickle.load(fd)\n",
    "\n",
    "    return val_data, gt_data_val, sim_data, gt_data\n",
    "\n",
    "### Test boolean is True if we want both input and target. Otherwise only input \n",
    "def get_input_target(input_data, target=[], infoids=[], test=False, normalize=False):\n",
    "    result_input = []\n",
    "    result_target = []\n",
    "    for infoid in infoids:\n",
    "\n",
    "        sample = []\n",
    "\n",
    "        models = sorted(list(input_data.keys()))\n",
    "\n",
    "        ### create one-hot encoding vector\n",
    "        Tindex=infoids.index(infoid)\n",
    "        narrative_binary=np.zeros(len(infoids))\n",
    "        narrative_binary[Tindex]=1\n",
    "\n",
    "        for model in models:\n",
    "            s = input_data[model][infoid]\n",
    "            sample.append(s)\n",
    "\n",
    "        sim_days = len(s)\n",
    "        ### Generate encodings equal to number of samples (number of days)\n",
    "        narrative_binary=np.tile(narrative_binary,(sim_days,1))\n",
    "        \n",
    "        ### Transpose matrix and concat one-hot encodings per frame\n",
    "        sample = np.array(sample)\n",
    "        sample = sample.transpose()\n",
    "#         print(sample)\n",
    "\n",
    "        sample = np.concatenate([sample, narrative_binary], axis=1)\n",
    "#         print(sample.shape)\n",
    "\n",
    "        result_input.extend(sample)\n",
    "        \n",
    "        if not test:\n",
    "            t = np.array(target[infoid])\n",
    "            result_target.extend(t)\n",
    "    \n",
    "    data_X = np.array(result_input)\n",
    "    data_X = data_X.astype(np.float64)\n",
    "    \n",
    "    number_features_to_scale = data_X.shape[-1] - len(infoids)\n",
    "    if normalize:\n",
    "        data_X[:,:number_features_to_scale] = np.log1p(data_X[:,:number_features_to_scale])\n",
    "    \n",
    "    if test:\n",
    "        return data_X\n",
    "    else:\n",
    "        data_y = np.array(result_target)\n",
    "        data_y = data_y.astype(np.float64)\n",
    "        if normalize:\n",
    "            data_y = np.log1p(data_y)\n",
    "        return data_X, data_y\n",
    "    \n",
    "def run_predictions(test_X, model, infoids=[], normalize=False):\n",
    "    \n",
    "    n_infoids = len(infoids)\n",
    "    sim_X = np.split(test_X, n_infoids)\n",
    "    \n",
    "    ml_preds = {}\n",
    "    \n",
    "    for infoid in infoids:\n",
    "    \n",
    "        Tindex=infoids.index(infoid)\n",
    "        sample = sim_X[Tindex]\n",
    "\n",
    "        y_hat = model.predict(sample)\n",
    "        \n",
    "        if normalize:\n",
    "            y_hat = np.round(np.expm1(y_hat))\n",
    "        else:\n",
    "            y_hat = np.round(y_hat)\n",
    "#             y_hat[y_hat<0] = 0\n",
    "            y_hat = np.absolute(y_hat)\n",
    "\n",
    "        ml_preds[infoid] = y_hat\n",
    "    \n",
    "    return ml_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_period = \"2020-11-09\"\n",
    "end_period = \"2020-11-29\"\n",
    "\n",
    "platform = \"platform_name\"\n",
    "\n",
    "task = \"total_shares\"\n",
    "\n",
    "domain = \"domain_name\"\n",
    "\n",
    "model_type = \"local_topics\"\n",
    "\n",
    "save_shares_path = \"../MCAS/ml_output/{2}/{0}/{3}/{1}/\".format(platform, task, domain, model_type)\n",
    "\n",
    "gt_path_shares = glob.glob(\"../MCAS/ml_output/{2}/{0}/{3}/{1}/LSTM*\".format(platform, task, domain, model_type))[0]\n",
    "\n",
    "paths_week_shares = glob.glob(\"../MCAS/ml_output/{2}/{0}/{3}/{1}/LSTM*\".format(platform, task, domain, model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perf, val_data, sim_data = helper_performance(paths=paths_week_shares, start_sim_period=start_period,\n",
    "                                                                end_sim_period=end_period, evaluation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ens_data = ensemble_all_average(paths_week_shares, gt_path=gt_path_shares, model_id=\"MCAS-Ensemble\", evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(save_shares_path+\"MCAS-ENSEMBLE/\")\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "with open(save_shares_path+\"MCAS-ENSEMBLE/simulations_data.pkl.gz\",'wb') as f:\n",
    "    pickle.dump(ens_data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blended-Exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"RMSE\"\n",
    "val_perf_ = val_perf.query(\"MODEL!='Internal'\").reset_index(drop=True)\n",
    "val_perf_blend,sim_blend_data = run_blender(val_perf=val_perf_, sim_data=sim_data,\n",
    "                                                             sim_perf=\"\", metric=metric, evaluation=False,model_id=\"MCAS-Blended-Exog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(save_shares_path+\"MCAS-BLENDED-EXOG/\")\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "with open(save_shares_path+\"MCAS-BLENDED-EXOG/simulations_data.pkl.gz\",'wb') as f:\n",
    "    pickle.dump(sim_blend_data,f)\n",
    "    \n",
    "val_perf_blend.to_pickle(save_shares_path+\"MCAS-BLENDED-EXOG/Gperformance_validation.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blended-Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"RMSE\"\n",
    "val_perf_ = val_perf.query(\"MODEL=='Internal'\").reset_index(drop=True)\n",
    "val_perf_blend_int,sim_blend_data_int = run_blender(val_perf=val_perf_, sim_data=sim_data,\n",
    "                                                                         sim_perf=\"\", metric=metric, evaluation=False,model_id=\"MCAS-Blended-Int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.mkdir(save_shares_path+\"MCAS-BLENDED-INTERNAL/\")\n",
    "except OSError as error:\n",
    "    print(error)  \n",
    "\n",
    "with open(save_shares_path+\"MCAS-BLENDED-INTERNAL/simulations_data.pkl.gz\",'wb') as f:\n",
    "    pickle.dump(sim_blend_data_int,f)\n",
    "    \n",
    "val_perf_blend_int.to_pickle(save_shares_path+\"MCAS-BLENDED-INTERNAL/Gperformance_validation.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sim = start_period\n",
    "end_sim = end_period\n",
    "\n",
    "prev_domain = \"\"\n",
    "\n",
    "### Here we need to load up any previous data_X and data_Y only for retraining purposes\n",
    "prev_path = \"/data/CP5_MCAS/MCAS/ml_output/{0}/{1}/{2}/{3}/MCAS-META-ENSEMBLE/\".format(prev_domain, platform, model_type,task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_gt, sim_data, sim_gt = get_validation_data(paths_week_shares, start_sim_period=start_period, end_sim_period=end_period, simulation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoids = sorted(list(val_gt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X, data_y = get_input_target(val_data, val_gt, infoids=infoids, test=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.shape, data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = get_input_target(sim_data, infoids=infoids, test=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(prev_path+\"data_x.pkl\", \"rb\") as f:\n",
    "        old_data_X = pickle.load(f)\n",
    "        \n",
    "    with open(prev_path+\"data_y.pkl\", \"rb\") as f:\n",
    "        old_data_y = pickle.load(f)\n",
    "    print(old_data_X.shape, old_data_y.shape)\n",
    "    data_X = np.concatenate((old_data_X,data_X),axis=0)\n",
    "    data_y = np.concatenate((old_data_y,data_y),axis=0)\n",
    "    print(data_X.shape, data_y.shape)\n",
    "except:\n",
    "    print(\"No previous input/output files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [.03, 0.05, .07, 0.1], #so called `eta` value\n",
    "              'max_depth': [3, 5, 6, 7, 10],\n",
    "              'min_child_weight': [1, 2, 3, 4],\n",
    "              'subsample': [0.7, 0.8, 1.0],\n",
    "              'colsample_bytree': [0.5, 0.7, 0.8, 1.0],\n",
    "              'n_estimators': [500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_grid = GridSearchCV(model,\n",
    "                        parameters,\n",
    "                        cv = 5,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "xgb_grid.fit(data_X,data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_grid.best_params_)\n",
    "new_params = xgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(**new_params)\n",
    "model.fit(data_X, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_preds = run_predictions(test_X, model=model, infoids=infoids, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(save_shares_path+\"MCAS-META-ENSEMBLE/\")\n",
    "except OSError as error:\n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(save_shares_path+\"MCAS-META-ENSEMBLE/best_model_params.pkl\", 'wb') as handle:\n",
    "    pickle.dump(new_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_shares_path+\"MCAS-META-ENSEMBLE/simulations_data.pkl.gz\",'wb') as f:\n",
    "    pickle.dump(ml_preds,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_shares_path+'MCAS-META-ENSEMBLE/model.pkl','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_shares_path+'MCAS-META-ENSEMBLE/data_x.pkl','wb') as f:\n",
    "    pickle.dump(data_X,f)\n",
    "    \n",
    "with open(save_shares_path+'MCAS-META-ENSEMBLE/data_y.pkl','wb') as f:\n",
    "    pickle.dump(data_y,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pnnl_socialsim]",
   "language": "python",
   "name": "conda-env-pnnl_socialsim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
